{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5e3688-9aef-4984-8739-ba9988cdd6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5564f4c-e306-425a-abcb-da62d7bafcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset loader\n",
    "from src.data.dataset import load_dataset, DATA_PATHS\n",
    "\n",
    "# load the raw articles\n",
    "dataset = load_dataset(DATA_PATHS[\"processed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c4337c-6095-4183-aeff-098116e3f2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.DataFrame(dataset)\n",
    "data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580f4a4d-0a60-4ad4-9018-22010f7ce415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b25f143-0849-4e96-a945-b36851d49e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNBA = []\n",
    "\n",
    "#selecting news from x date to y date\n",
    "import datetime\n",
    "x = datetime.datetime(2020, 10, 1)  #from this date\n",
    "y = datetime.datetime(2020, 10, 31) #to this date\n",
    "for a in dataset:\n",
    "    if a[\"date\"]>= x and a[\"date\"] <= y:\n",
    "        dataNBA.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbde6d1f-d285-40f6-bcdb-9784389fab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNBA = pd.DataFrame(dataNBA)\n",
    "\n",
    "#selecting news from preselected concepts (in this case: basketball OR nba)\n",
    "selection = [\"basketball\", \"nba\"]  \n",
    "mask = dataNBA.concepts.apply(lambda x: any(item for item in selection if item in x))\n",
    "dataNBA = dataNBA[mask]\n",
    "len(dataNBA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adb1c53-188d-48b8-ad2d-292d98d01dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting only english articles\n",
    "selection = [\"eng\"]  \n",
    "mask = dataNBA.lang.apply(lambda x: any(item for item in selection if item in x))\n",
    "dataNBA = dataNBA[mask]\n",
    "len(dataNBA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312e7aeb-e71f-41c9-8d60-966ea3f9efbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNBA.lang.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1191ed-97eb-4732-ae98-70b471a1b169",
   "metadata": {},
   "source": [
    "# Finding and deleting duplicate articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee076b5-01cb-4866-a8dd-174a47b27c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# From dataframe deletes all duplicates\n",
    "def del_duplicates(dataframe):\n",
    "    corpus = dataframe[\"body\"].tolist()\n",
    "    \n",
    "    # Create a TfidfVectorizer object and fit it to the corpus\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # Compute the cosine similarity between all pairs of documents\n",
    "    cos_sim = cosine_similarity(tfidf)\n",
    "    \n",
    "    # Creating a list of duplicates with index pointer\n",
    "    x = []\n",
    "    threshold = 0.8\n",
    "    for i in range(len(corpus)):\n",
    "        for j in range(i+1, len(corpus)):\n",
    "            if cos_sim[i,j] > threshold:\n",
    "                if len([item for item in x if item[1] == i]) == 0:\n",
    "                    x.append((i,j,cos_sim[i,j]))\n",
    "                    \n",
    "    \n",
    "    #adding duplicate column\n",
    "    dataframe['duplicate_of'] = np.nan\n",
    "    \n",
    "    dataframe = dataframe.reset_index(drop=True)\n",
    "    \n",
    "    keep_cols = ['title', 'body', 'duplicate_of','dateTime','date','eventUri']\n",
    "    dataframe = dataframe[dataframe.columns.intersection(keep_cols)]\n",
    "    \n",
    "    for duplicate in x:\n",
    "        DuplicateIndex = duplicate[1]\n",
    "        dataframe.at[DuplicateIndex, \"duplicate_of\"] = duplicate[0]\n",
    "    \n",
    "    return dataframe[np.isnan(dataframe['duplicate_of'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ddc6ca-1c20-414d-bace-95933dcc6dd7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Binary divison with K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceb69a9-9bf7-4d9e-8819-bbb041b95f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def k_means(dataframe):\n",
    "    if 'embeddings' not in dataframe.columns:    \n",
    "        dataframeBodyList = dataframe[\"body\"].tolist()\n",
    "        model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "        embeddings = model.encode(dataframeBodyList)\n",
    "        dataframe['embeddings'] = embeddings.tolist()\n",
    "\n",
    "    dataframe['cluster'] = [[-1] for _ in range(len(dataframe))]\n",
    "    clusterMYDataframe(dataframe)\n",
    "    clusterNumb(dataframe)\n",
    "    dataframe.cluster = dataframe.cluster.apply(lambda x: x[-1]) \n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def KmeansCluster(Dataframe):\n",
    "    embeddings = Dataframe[\"embeddings\"].tolist()\n",
    "    clusters = KMeans(n_clusters=2, random_state=42).fit(embeddings)\n",
    "    clusters = clusters.labels_.tolist()\n",
    "    \n",
    "    addToClusterColumn(Dataframe, clusters)\n",
    "    \n",
    "    dataOne = Dataframe[Dataframe.cluster.str[-1] == 0] \n",
    "    dataTwo = Dataframe[Dataframe.cluster.str[-1] == 1]\n",
    "    \n",
    "    return dataOne, dataTwo, cosine(embeddings)\n",
    "\n",
    "# Calculates silhouette score\n",
    "def silhouette(clusters, embeddings):\n",
    "    score = silhouette_score(embeddings, clusters, metric = \"cosine\")\n",
    "    if score <= 0.5:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Calculates cosine similarity\n",
    "def cosine(embeddings):\n",
    "    distance = find_average_distance_to_centroid(embeddings)\n",
    "    if distance > 0.1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Return True if cluster has 5 or more articles\n",
    "def condition(Dataframe):\n",
    "    NumOfDocs = Dataframe.shape[0]\n",
    "    if NumOfDocs >= 5:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def addToClusterColumn(Dataframe, clusters):\n",
    "    y = 0\n",
    "    for val in Dataframe.cluster:\n",
    "        val.append(clusters[y])\n",
    "        y+=1\n",
    "        \n",
    "def clusterMYDataframe(Dataframe):\n",
    "    dataOne, dataTwo, condi = KmeansCluster(Dataframe)\n",
    "    if condition(dataOne) and condi:\n",
    "        clusterMYDataframe(dataOne)\n",
    "    if condition(dataTwo) and condi:\n",
    "        clusterMYDataframe(dataTwo)\n",
    "\n",
    "        \n",
    "#CLUSTER NUMBER\n",
    "def MaxListLength(Dataframe):\n",
    "    length = 0\n",
    "    y = 0\n",
    "    for val in Dataframe.cluster:\n",
    "        if len(val) > length:\n",
    "            length = len(val)\n",
    "        y+=1\n",
    "    return length\n",
    "\n",
    "#this is used to create a unique decimal number by adding zeroos to binary numbers that are > of the MAX binary number\n",
    "def ZerosToList(mylist, maxLength):\n",
    "    x = len(mylist)\n",
    "    N = maxLength - x\n",
    "    my_array = np.asarray(mylist)\n",
    "    my_array = np.pad(my_array, (0, N), 'constant')\n",
    "    mylist = list(my_array)\n",
    "    return mylist\n",
    "\n",
    "def clusterNumb(Dataframe):\n",
    "    maxListLen = MaxListLength(Dataframe)\n",
    "    y = 0\n",
    "    for val in Dataframe.cluster:\n",
    "        val.pop(0)\n",
    "        Binar = ZerosToList(val, maxListLen)\n",
    "        res = int(\"\".join(str(x) for x in Binar), 2)\n",
    "        val.append(res)\n",
    "        y+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef82aec-f1c5-41db-ab70-5c7a63912670",
   "metadata": {},
   "source": [
    "# TIME window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec87771-f87f-43b3-8561-5c8a4a6e5675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "def TimeWindow(dataframe, IndexDateLIst, NumOfDays):\n",
    "    max_window_start = None\n",
    "    max_window_end = None\n",
    "    max_window_count = 0\n",
    "\n",
    "    # Sort the list of tuples by date\n",
    "    sorted_list = sorted(IndexDateLIst, key=lambda x: x[1])\n",
    "\n",
    "    # Loop through the sorted list of tuples and count the number of indexes within each (NumOfDays) window\n",
    "    for i in range(len(sorted_list)):\n",
    "        window_start = sorted_list[i][1]\n",
    "        window_end = window_start + timedelta(days=NumOfDays)\n",
    "        window_count = 1\n",
    "        for j in range(i+1, len(sorted_list)):\n",
    "            if sorted_list[j][1] >= window_end:\n",
    "                break\n",
    "            window_count += 1\n",
    "\n",
    "        if window_count > max_window_count:\n",
    "            max_window_count = window_count\n",
    "            max_window_start = window_start\n",
    "            max_window_end = window_end\n",
    "\n",
    "    # List of index-es of documents published outside the time window\n",
    "    sorted_list = [x[0] for x in sorted_list if x[1] < max_window_start or x[1] > max_window_end]\n",
    "\n",
    "    dataframe = dataframe.drop(index=sorted_list)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def Time(dataframe, NumOfDays):\n",
    "    clusters = dataframe.cluster.unique()\n",
    "    for ClusterID in clusters:\n",
    "        IndexDateLIst = list(zip(dataframe[dataframe['cluster'] == ClusterID].index, dataframe[dataframe.cluster == ClusterID].date))\n",
    "        dataframe = TimeWindow(dataframe, IndexDateLIst, NumOfDays)\n",
    "    return dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf4f419-c3f2-44cb-bc09-12202e571606",
   "metadata": {},
   "source": [
    "# Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4feb00-e381-4006-b464-21d68afb7d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_centroid(embeddings):\n",
    "    centroid = np.mean(embeddings, axis=0)\n",
    "    return centroid\n",
    "\n",
    "def find_average_distance_to_centroid(embeddings):\n",
    "    # Find the centroid of the embeddings\n",
    "    centroid = find_centroid(embeddings)\n",
    "    # Compute the cosine similarities between each embedding and the centroid\n",
    "    similarities = cosine_similarity(embeddings, centroid.reshape(1, -1)).flatten()\n",
    "    # Compute the distances between each embedding and the centroid\n",
    "    distances = 1 - similarities\n",
    "    # Compute the average distance\n",
    "    avg_distance = np.mean(distances)\n",
    "    return avg_distance\n",
    "\n",
    "def inside_cluster_sim(dataframe):\n",
    "    delClusters = []\n",
    "    clusters = dataframe.cluster.unique()\n",
    "    for cluster in clusters:\n",
    "        embedds = dataframe[dataframe.cluster == cluster][\"embeddings\"].tolist()\n",
    "        if len(dataframe[dataframe.cluster == cluster]) == 1:\n",
    "            delClusters.append(cluster)\n",
    "            continue\n",
    "        if len(dataframe[dataframe.cluster == cluster]) == 2:\n",
    "            if cosine_similarity(embedds, embedds)[0][1] < 0.85:\n",
    "                delClusters.append(cluster)\n",
    "                continue\n",
    "\n",
    "        distance = find_average_distance_to_centroid(embedds)\n",
    "        if distance > 0.2:\n",
    "            delClusters.append(cluster)\n",
    "\n",
    "    return dataframe[dataframe.cluster.isin(delClusters) == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d01cc7-02b9-4176-a1e0-caaf50859a6e",
   "metadata": {},
   "source": [
    "# NER (Named entity recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb77a4d-6bd4-4cbf-a93c-fb7b9501d6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating jaccard similarity\n",
    "def jaccard_similarity(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(set(list1)) + len(set(list2))) - intersection\n",
    "    return float(intersection) / union\n",
    "\n",
    "\n",
    "def cluster_ents(cluster, dataframe):\n",
    "    even = dataframe[dataframe.cluster == cluster][\"body\"]\n",
    "    index_of_body = dataframe[dataframe.cluster == cluster].index\n",
    "    entitete = []\n",
    "    for a, b in zip(even, index_of_body):\n",
    "        doc = nlp(a)\n",
    "        entitete.append([b, [sent.text.strip() for sent in doc.ents]])\n",
    "    return entitete\n",
    "\n",
    "\n",
    "def cluster_jaccard_similarity(entitete):\n",
    "    Inside_cluster_news_jaccard_similarity = []\n",
    "    delRows = []\n",
    "    for i in range(len(entitete)):\n",
    "        simil = 0\n",
    "        for j in range(len(entitete)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            similarity = jaccard_similarity(entitete[i][1], entitete[j][1])\n",
    "            simil = simil + similarity\n",
    "            \n",
    "        simil = simil/(len(entitete)-1)\n",
    "        \n",
    "        if simil < 0.1:\n",
    "            delRows.append(entitete[i][0])\n",
    "        Inside_cluster_news_jaccard_similarity.append([entitete[i][0],simil])\n",
    "    return delRows\n",
    "\n",
    "\n",
    "def NERjaccard(dataframe):\n",
    "    delRows_NER = []\n",
    "    clusters = dataframe.cluster.unique()\n",
    "    \n",
    "    for cluster in clusters:            \n",
    "        entitete = cluster_ents(cluster, dataframe)\n",
    "        Inside_cluster_news_jaccard_similarity = cluster_jaccard_similarity(entitete)\n",
    "\n",
    "        delRows_NER.extend(Inside_cluster_news_jaccard_similarity)\n",
    "        \n",
    "    return dataframe[dataframe.index.isin(delRows_NER) == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379b305d-73ec-402c-9d64-0bb836b9ca7a",
   "metadata": {},
   "source": [
    "# Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a449d-7c4e-44f0-b08b-ca4f8d1916f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class TextFormat:\n",
    "    @staticmethod\n",
    "    def normalize_whitespaces(text: str):\n",
    "        return re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    @staticmethod\n",
    "    def strip_trailing_whitespaces(text: str):\n",
    "        return text.strip()\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text: str):\n",
    "        text = TextFormat.normalize_whitespaces(text)\n",
    "        text = TextFormat.strip_trailing_whitespaces(text)\n",
    "        return text\n",
    "    \n",
    "def clean_body(dataframe):\n",
    "    dataframe[\"body\"] = dataframe[\"body\"].apply(lambda x: TextFormat.clean_text(x))\n",
    "    dataframe[\"title\"] = dataframe[\"title\"].apply(lambda x: TextFormat.clean_text(x))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db3400-0ce0-4da3-9d56-829dc60dec42",
   "metadata": {},
   "source": [
    "# ENTIRE pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e7a0e0-d558-40a2-9843-b72f3b3a022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_by_event(dataframe, days):\n",
    "    dataframe = del_duplicates(dataframe)\n",
    "    orig = dataframe.copy(deep=True)\n",
    "    clustered = pd.DataFrame()\n",
    "    for _ in tqdm(range(1)):\n",
    "        if _ == 0:\n",
    "            dataframe = k_means(dataframe)\n",
    "            dataframe = Time(dataframe, days)\n",
    "            dataframe = inside_cluster_sim(dataframe)\n",
    "            dataframe = NERjaccard(dataframe)\n",
    "            clustered = dataframe.copy(deep=True)\n",
    "        else:\n",
    "            dataframe = orig.drop(clustered.index)\n",
    "            dataframe = k_means(dataframe)\n",
    "            dataframe = Time(dataframe, days)\n",
    "            dataframe = inside_cluster_sim(dataframe)\n",
    "            dataframe = NERjaccard(dataframe)\n",
    "            clustered = pd.concat([clustered, dataframe]).sort_index()\n",
    "    return clustered          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb70452c-c25a-4939-adbd-309d35a2db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = cluster_by_event(dataNBA, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83a3149-1948-4555-828c-210be0ecea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
